{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5a141f-ddab-4d70-95e2-76f9d03ed13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 44s]\n",
      "mean_absolute_error: 0.08044939115643501\n",
      "\n",
      "Best mean_absolute_error So Far: 0.06830994598567486\n",
      "Total elapsed time: 00h 03m 28s\n",
      "Model for NUBL saved at ./NULB/NUBL_tft_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tft/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 102 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure TensorFlow uses GPU if available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    df_stock = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'Date' column to datetime\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Remove '%' and convert 'Percent Change' to float, handling errors\n",
    "    df_stock['Percent Change'] = df_stock['Percent Change'].str.replace('%', '').apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Remove commas from 'Volume' and convert to float\n",
    "    df_stock['Volume'] = df_stock['Volume'].astype(str).str.replace(',', '').apply(pd.to_numeric, errors='coerce')\n",
    "    # Create additional features\n",
    "    df_stock['day_of_week'] = df_stock['Date'].dt.dayofweek\n",
    "    df_stock['month'] = df_stock['Date'].dt.month\n",
    "\n",
    "    # Select features and target\n",
    "    features = ['Close', 'day_of_week', 'month']\n",
    "    target = 'Close'\n",
    "\n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    df_stock[features] = scaler.fit_transform(df_stock[features])\n",
    "\n",
    "    # Save the scaler per stock\n",
    "    scaler_save_path = file_path.replace('.csv', '_scaler.pkl')\n",
    "    joblib.dump(scaler, scaler_save_path)\n",
    "\n",
    "    return df_stock, features, target, scaler\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - window_size):\n",
    "        X.append(df[features].iloc[i:i+window_size].values)\n",
    "        y.append(df[target].iloc[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        res = x + inputs\n",
    "\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "        return x + res\n",
    "\n",
    "def gated_residual_network(inputs, units): \n",
    "        x = layers.Dense(units, activation='relu')(inputs) \n",
    "        x = layers.Dense(inputs.shape[-1])(x) \n",
    "        gate = layers.Dense(inputs.shape[-1], activation='sigmoid')(inputs) \n",
    "        return x * gate + inputs\n",
    "\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "# Function to calculate the percentage error for each prediction\n",
    "def calculate_accuracy(y_true, y_pred, threshold=0.05):\n",
    "    # Calculate percentage error\n",
    "    percentage_error = np.abs((y_true - y_pred) / y_true)\n",
    "    \n",
    "    # Count how many predictions are within the threshold (e.g., 5%)\n",
    "    accurate_predictions = np.sum(percentage_error <= threshold)\n",
    "    \n",
    "    # Calculate the accuracy as the proportion of accurate predictions\n",
    "    accuracy = accurate_predictions / len(y_true)\n",
    "    return accuracy\n",
    "def build_model(hp, input_shape):\n",
    "        input_shape = (WINDOW_SIZE, 3)\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        x = gated_residual_network(x, units=hp.Int('grn_units', min_value=32, max_value=128, step=32))\n",
    "        x = layers.LSTM(units=50, return_sequences=True)(x)\n",
    "        x = layers.LSTM(units=50, return_sequences=True)(x)\n",
    "        for i in range(hp.Int('num_transformer_blocks', 2, 8, 2)):\n",
    "            x = transformer_encoder(\n",
    "                x,\n",
    "                head_size=hp.Int('head_size', 8, 256, 32),\n",
    "                num_heads=hp.Int('num_heads', 2, 16),\n",
    "                ff_dim=hp.Int('ff_dim', 4, 64),\n",
    "                dropout=hp.Float(f'dropout_{i}', 0.1, 0.6)\n",
    "            )\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        for i in range(hp.Int('num_mlp_layers', 1, 3)):\n",
    "            x = layers.Dense(hp.Int(f'mlp_units_{i}', 32, 256, 32))(x)\n",
    "            x = layers.Activation('relu')(x)\n",
    "            x = layers.Dropout(hp.Float(f'mlp_dropout_{i}', 0.1, 0.6))(x)\n",
    "        outputs = layers.Dense(1)(x)\n",
    "        model = models.Model(inputs, outputs)\n",
    "        optimizer_name = hp.Choice('optimizer', ['adam', 'adamax'])\n",
    "        learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_absolute_error', \n",
    "            metrics=['mean_absolute_error', 'mean_squared_error']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "# After predicting with your trained model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predict using the trained model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate various metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate accuracy (percentage within 5% threshold)\n",
    "    accuracy = calculate_accuracy(y_test, y_pred, threshold=0.05)\n",
    "\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"RÂ² Score: {r2}\")\n",
    "    print(f\"Accuracy (within 5%): {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return mae, mse, r2, accuracy\n",
    "\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train, input_shape):\n",
    "    tuner = RandomSearch(lambda hp: build_model(hp, input_shape),\n",
    "                         objective='mean_absolute_error',\n",
    "                         max_trials=5,\n",
    "                         executions_per_trial=2,\n",
    "                         directory='hyperparam_tuning',\n",
    "                         project_name='tft_stock')\n",
    "\n",
    "    tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(patience=5)])\n",
    "    \n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    # Train the best model again on full training data and store history\n",
    "    history = best_model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(patience=5)])\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    metrics = ['loss', 'mean_absolute_error', 'mean_squared_error']\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric.replace('_', ' ').title())\n",
    "        plt.legend()\n",
    "        plt.title(f'{metric.replace(\"_\", \" \").title()} Over Epochs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example of using it after training a model\n",
    "def train_and_evaluate(data_directory):\n",
    "    for file_name in os.listdir(data_directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            stock_name = os.path.splitext(file_name)[0]\n",
    "            file_path = os.path.join(data_directory, file_name)\n",
    "            print(f\"Processing {stock_name}...\")\n",
    "\n",
    "            # Preprocess data\n",
    "            df, features, target, scaler = preprocess_data(file_path)\n",
    "\n",
    "            # Prepare sequences\n",
    "            X, y = df_to_X_y(df, features, target)\n",
    "\n",
    "            if len(X) == 0:\n",
    "                print(f\"Not enough data to train for {stock_name}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Split data into training and test sets (80% training, 20% testing)\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = X[:train_size], X[train_size:]\n",
    "            y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "            # Train the model\n",
    "            input_shape = (X.shape[1], X.shape[2])\n",
    "            model, history = train_model(X_train, y_train, input_shape)\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "            evaluate_model(model, X_test, y_test)\n",
    "\n",
    "            plot_training_history(history)\n",
    "\n",
    "            # Save the model for later use\n",
    "            model_save_path = os.path.join(data_directory, f\"{stock_name}_tft_model.keras\")\n",
    "            model.save(model_save_path)\n",
    "            print(f\"Model for {stock_name} saved at {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = './NULB/'  # Replace with the actual path\n",
    "    train_and_evaluate(data_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab18b7-7afd-455b-b8dd-f316d0327670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
