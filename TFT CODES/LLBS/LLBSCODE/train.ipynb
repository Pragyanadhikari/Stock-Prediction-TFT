{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4fe3e0-ba90-41fb-82d7-06f1a77ade2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "import joblib\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "\n",
    "def code(outside_path):\n",
    "\n",
    "    df_stock = pd.read_csv(outside_path, parse_dates=['Date'], dayfirst=True)\n",
    "    df_stock['Open'] = df_stock['Open'].astype(str)\n",
    "    df_stock['Open'] = df_stock['Open'].str.replace(',', '').astype(float)\n",
    "    df_stock['Date'] = pd.to_datetime(df_stock['Date'], format='%d/%m/%Y')\n",
    "    df_stock['day_of_week'] = df_stock['Date'].dt.dayofweek\n",
    "    df_stock['month'] = df_stock['Date'].dt.month\n",
    "\n",
    "    features = ['Date', 'Open', 'day_of_week', 'month']\n",
    "    df_stock = df_stock[features]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_stock[['Open', 'day_of_week', 'month']]), columns=['Open', 'day_of_week', 'month'])\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "    def df_to_X_y(df, window_size=5):\n",
    "        df_as_np = df.to_numpy()\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(df_as_np) - window_size):\n",
    "            X.append(df_as_np[i:i + window_size])\n",
    "            y.append(df_as_np[i + window_size][0])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    WINDOW_SIZE = 6\n",
    "    X, y = df_to_X_y(df_scaled, WINDOW_SIZE)\n",
    "\n",
    "    def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        res = x + inputs\n",
    "\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "        return x + res\n",
    "\n",
    "    def gated_residual_network(inputs, units): \n",
    "        x = layers.Dense(units, activation='relu')(inputs) \n",
    "        x = layers.Dense(inputs.shape[-1])(x) \n",
    "        gate = layers.Dense(inputs.shape[-1], activation='sigmoid')(inputs) \n",
    "        return x * gate + inputs\n",
    "\n",
    "    def build_tft_model(hp):\n",
    "        input_shape = (WINDOW_SIZE, 3)\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = inputs\n",
    "        x = gated_residual_network(x, units=hp.Int('grn_units', min_value=32, max_value=128, step=32))\n",
    "        x = layers.LSTM(units=50, return_sequences=True)(x)\n",
    "        x = layers.LSTM(units=50, return_sequences=True)(x)\n",
    "        for i in range(hp.Int('num_transformer_blocks', 2, 8, 2)):\n",
    "            x = transformer_encoder(\n",
    "                x,\n",
    "                head_size=hp.Int('head_size', 8, 256, 32),\n",
    "                num_heads=hp.Int('num_heads', 2, 16),\n",
    "                ff_dim=hp.Int('ff_dim', 4, 64),\n",
    "                dropout=hp.Float(f'dropout_{i}', 0.1, 0.6)\n",
    "            )\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        for i in range(hp.Int('num_mlp_layers', 1, 3)):\n",
    "            x = layers.Dense(hp.Int(f'mlp_units_{i}', 32, 256, 32))(x)\n",
    "            x = layers.Activation('relu')(x)\n",
    "            x = layers.Dropout(hp.Float(f'mlp_dropout_{i}', 0.1, 0.6))(x)\n",
    "        outputs = layers.Dense(1)(x)\n",
    "        model = models.Model(inputs, outputs)\n",
    "        optimizer_name = hp.Choice('optimizer', ['adam', 'adamax'])\n",
    "        learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')\n",
    "        if optimizer_name == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "        model.compile(\n",
    "            optimizer=optimizer, \n",
    "            loss='mean_absolute_error', \n",
    "            metrics=['mean_absolute_error', 'mean_squared_error']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_tft_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=30,\n",
    "        directory='./tft_tuning',\n",
    "        project_name='tft_project'\n",
    "    )\n",
    "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    model = build_tft_model(best_hps)\n",
    "\n",
    "    def plot_loss(history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_mae(history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if 'mean_absolute_error' in history.history and 'val_mean_absolute_error' in history.history:\n",
    "            plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
    "            plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "            plt.title('Training and Validation MAE')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('MAE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"MAE metric not found in history.\")\n",
    "\n",
    "    def plot_mse(history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if 'mean_squared_error' in history.history and 'val_mean_squared_error' in history.history:\n",
    "            plt.plot(history.history['mean_squared_error'], label='Training MSE')\n",
    "            plt.plot(history.history['val_mean_squared_error'], label='Validation MSE')\n",
    "            plt.title('Training and Validation MSE')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"MSE metric not found in history.\")\n",
    "    \n",
    "    best_model_hyperparameters = None\n",
    "    best_model_mae = float('inf')\n",
    "    worst_model_hyperparameters = None\n",
    "    worst_model_mae = float('-inf')\n",
    "\n",
    "    nested_scores = []\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=100)\n",
    "\n",
    "    for fold_outer_idx, (train_ix, test_ix) in enumerate(outer_cv.split(X)):\n",
    "        X_train_outer, X_test_outer = X[train_ix], X[test_ix]\n",
    "        y_train_outer, y_test_outer = y[train_ix], y[test_ix]\n",
    "        inner_cv = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "        for fold_idx, (train_ix_inner, val_ix) in enumerate(inner_cv.split(X_train_outer)):\n",
    "            X_train_inner, X_val = X_train_outer[train_ix_inner], X_train_outer[val_ix]\n",
    "            y_train_inner, y_val = y_train_outer[train_ix_inner], y_train_outer[val_ix]\n",
    "            tuner = kt.RandomSearch(\n",
    "                build_tft_model,\n",
    "                objective='val_loss',\n",
    "                max_trials=5,\n",
    "                directory=f'./keras_tuner_random_dir_fold_tf_{fold_outer_idx}_{fold_idx}',\n",
    "                project_name=f'hyperparameter_random_tuning_fold_tf_{fold_outer_idx}_{fold_idx}'\n",
    "            )\n",
    "            tuner.search(X_train_inner, y_train_inner, validation_data=(X_val, y_val), epochs=5)\n",
    "            best_hps = tuner.oracle.get_best_trials(1)[0].hyperparameters\n",
    "            print(f\"Best hyperparameters for fold {fold_idx}: {best_hps}\")\n",
    "            model = build_tft_model(best_hps)\n",
    "            es = [keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "            history = model.fit(X_train_inner, y_train_inner, validation_data=(X_val, y_val), epochs=5, batch_size=32, callbacks=es)\n",
    "            plot_loss(history)\n",
    "            plot_mae(history)\n",
    "            plot_mse(history)\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            if mae < best_model_mae:\n",
    "                best_model_mae = mae\n",
    "                best_model_hyperparameters = best_hps\n",
    "                best_model = model\n",
    "            if mae > worst_model_mae:\n",
    "                worst_model_mae = mae\n",
    "                worst_model_hyperparameters = best_hps\n",
    "                worst_model = model\n",
    "                start_time = time.time()\n",
    "        y_pred_best = best_model.predict(X_test_outer)\n",
    "        y_pred_worst = worst_model.predict(X_test_outer)\n",
    "        end_time = time.time()\n",
    "        mse_best = mean_squared_error(y_test_outer, y_pred_best)\n",
    "        mae_best = mean_absolute_error(y_test_outer, y_pred_best)\n",
    "        rmse_best = np.sqrt(mse_best)\n",
    "        r2_best = r2_score(y_test_outer, y_pred_best)\n",
    "        time_duration = end_time - start_time\n",
    "        mse_worst = mean_squared_error(y_test_outer, y_pred_worst)\n",
    "        mae_worst = mean_absolute_error(y_test_outer, y_pred_worst)\n",
    "        rmse_worst = np.sqrt(mse_worst)\n",
    "        r2_worst = r2_score(y_test_outer, y_pred_worst)\n",
    "        nested_scores.append({\n",
    "            \"Best Model\": {\n",
    "                \"MSE\": mse_best,\n",
    "                \"R^2\": r2_best,\n",
    "                \"RMSE\": rmse_best,\n",
    "                \"MAE\": mae_best,\n",
    "                \"testing time\": time_duration\n",
    "            },\n",
    "            \"Worst Model\": {\n",
    "                \"MSE\": mse_worst,\n",
    "                \"R^2\": r2_worst,\n",
    "                \"RMSE\": rmse_worst,\n",
    "                \"MAE\": mae_worst\n",
    "            }\n",
    "        })\n",
    "\n",
    "    print(\"Nested Cross-Validation Scores:\")\n",
    "    for idx, score in enumerate(nested_scores):\n",
    "        print(f\"Fold {idx+1}:\")\n",
    "        print(f\"  Best Model - MSE: {score['Best Model']['MSE']}, R^2: {score['Best Model']['R^2']}, RMSE: {score['Best Model']['RMSE']}, MAE: {score['Best Model']['MAE']}, Testing Time: {score['Best Model']['testing time']}\")\n",
    "        print(f\"  Worst Model - MSE: {score['Worst Model']['MSE']}, R^2: {score['Worst Model']['R^2']}, RMSE: {score['Worst Model']['RMSE']}, MAE: {score['Worst Model']['MAE']}\")\n",
    "\n",
    "    avg_best_mse = np.mean([score['Best Model']['MSE'] for score in nested_scores])\n",
    "    avg_best_mae = np.mean([score['Best Model']['MAE'] for score in nested_scores])\n",
    "    avg_best_r2 = np.mean([score['Best Model']['R^2'] for score in nested_scores])\n",
    "    avg_best_rmse = np.mean([score['Best Model']['RMSE'] for score in nested_scores])\n",
    "\n",
    "    print(\"\\nAverage Best Model Metrics across all folds:\")\n",
    "    print(f\"Average MSE: {avg_best_mse}\")\n",
    "    print(f\"Average MAE: {avg_best_mae}\")\n",
    "    print(f\"Average R^2: {avg_best_r2}\")\n",
    "    print(f\"Average RMSE: {avg_best_rmse}\")\n",
    "\n",
    "    y_pred = best_model.predict(X).flatten()\n",
    "    y_observed = y.flatten()\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(y_observed, y_pred, color='blue', alpha=0.5)\n",
    "    plt.plot([min(y_observed), max(y_observed)], [min(y_observed), max(y_observed)], color='red', linestyle='--')\n",
    "    plt.title('Transformer - Observed vs Predicted')\n",
    "    plt.xlabel('Observed Ltp')\n",
    "    plt.ylabel('Predicted Ltp')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    optimizer = best_model.optimizer\n",
    "    learning_rate = float(optimizer.learning_rate.numpy())\n",
    "\n",
    "    print(\"Optimizer:\", type(optimizer).__name__)\n",
    "    print(\"Learning Rate:\", learning_rate)\n",
    "\n",
    "    def get_dropout_rate(layer):\n",
    "        if hasattr(layer, 'rate'):\n",
    "            return layer.rate\n",
    "        elif hasattr(layer, 'dropout'):\n",
    "            return layer.dropout\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    dropout_rates = []\n",
    "\n",
    "    for layer in best_model.layers:\n",
    "        rate = get_dropout_rate(layer)\n",
    "        if rate is not None:\n",
    "            dropout_rates.append((layer.name, rate))\n",
    "\n",
    "    print(\"Dropout Rates:\")\n",
    "    for layer_name, rate in dropout_rates:\n",
    "        print(f\"{layer_name}: {rate}\")\n",
    "\n",
    "    best_hyperparameters_dict = best_model_hyperparameters.values\n",
    "    worst_hyperparameters_dict = worst_model_hyperparameters.values\n",
    "\n",
    "    print(\"Best Hyperparameters:\", best_hyperparameters_dict)\n",
    "    print(\"Worst Hyperparameters:\", worst_hyperparameters_dict)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2938c39-a9dc-498b-aba6-b18f0e5911e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
